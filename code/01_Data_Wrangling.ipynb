{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e3c5b6f-b504-4a5c-a491-a624594faa46",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee10f11-e82b-4ce0-802b-87d264aa6a19",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d4abe61-4562-40bb-bd07-bb9461133009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f923a8-b73f-423a-8632-e3d791b86cef",
   "metadata": {},
   "source": [
    "Create my Reddit connection via my praw.ini file in this project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f18884-3dbf-4fcc-a2ac-6eea8445952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faab2e1-3ecb-46e4-8d37-40c5a6864be8",
   "metadata": {},
   "source": [
    "## Reddit Data Acquition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bcab35-4cbb-418c-a103-4bd16f9502c5",
   "metadata": {},
   "source": [
    "#### Collecting data from reddit via our api in our praw.ini file from 5 different subreddits and collecting the Title of the post, the questions asked, the subreddit, the answer which is the top ranked comment, and the time the post was created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002f090d-6ff2-4f75-8e79-ceb05bea66c3",
   "metadata": {},
   "source": [
    "#### *NOTE: What I am showing here is the refined code after several iterations of trying to read in the reddit data. I was not able to obtain all 5000 rows in one attempt as it kept timing out at about 800-1000. In actuality I had to do this process 5-6 diferent times in a broken down slower and shorter way. Had I not run into time out issue or was willing to wait several hours to pull this data I would run it this way below as I had originally intended. That messy code will be left out as from the last project it was not desired and wont make any sense if you actually look at it comapred to simply and solely this notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe56923b-47a7-4938-abb1-b311888d1455",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['techsupport','askphilosophy','askculinary','askacademia','askstatistics']\n",
    "\n",
    "data = pd.DataFrame(columns=['title', 'selftext', 'subreddit', 'comments', 'created_utc'])\n",
    "\n",
    "posts_per_subreddit = 1100\n",
    "\n",
    "for subreddit_name in subreddits:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    collecting_posts = subreddit.hot(limit=posts_per_subreddit)\n",
    "    \n",
    "    for post in collecting_posts:\n",
    "        if not post.over_18:\n",
    "            post.comments.replace_more(limit=0)\n",
    "            highest_ups = -1\n",
    "            best_comment = \"\"\n",
    "            for comment in post.comments:\n",
    "                if comment.ups > highest_ups:\n",
    "                    best_comment = comment.body\n",
    "                    highest_ups = comment.ups\n",
    "            \n",
    "            data = data.append({\n",
    "                'title': post.title,\n",
    "                'selftext': post.selftext,\n",
    "                'subreddit': post.subreddit.display_name,\n",
    "                'comments': best_comment,\n",
    "                'created_utc': post.created_utc\n",
    "            }, ignore_index=True)\n",
    "        time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7ed57-8832-4f01-a7cf-7e79604459f6",
   "metadata": {},
   "source": [
    "Saving our dataframe and exporting it into a csv file for safety reasons and to ensure we dont overwrite anything or make mistakes in the future. We will then read in this new csv file when obtaining chatgrp responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697865f-d86d-4fb4-946d-70ae21ef0e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('reddit_only_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda771e5-4092-4f29-8af7-0a42e887fb83",
   "metadata": {},
   "source": [
    "## OpenAI Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbef9f1-20fc-4788-8218-f8b806dac831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775fb292-518b-456d-a83d-25e8a3dd9140",
   "metadata": {},
   "source": [
    "Currently the dataframe below is the data with just reddit information, the chatgpt responses will be added next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952a0032-68c1-4ede-9297-4e52889cc81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comments</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Recommended wiki articles (including malware r...</td>\n",
       "      <td>## Check out these recommended threads on our ...</td>\n",
       "      <td>techsupport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.627263e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Another update on our future</td>\n",
       "      <td>Dear /r/TechSupport visitors and subscribers,\\...</td>\n",
       "      <td>techsupport</td>\n",
       "      <td>Discord is definitely not a proper alternative...</td>\n",
       "      <td>1.690183e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Help for my 92yo blind grandad</td>\n",
       "      <td>Hi Reddit I hope this might reach the right pe...</td>\n",
       "      <td>techsupport</td>\n",
       "      <td>I know this isn't what you want to hear, but I...</td>\n",
       "      <td>1.695145e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Moving countries and needing to be sure “illeg...</td>\n",
       "      <td>Heyho, I’m moving countries in a couple of mon...</td>\n",
       "      <td>techsupport</td>\n",
       "      <td>Most data isn’t deleted when you click delete....</td>\n",
       "      <td>1.695140e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Insanely High PC DPC-Count Latency - How To Fix?</td>\n",
       "      <td>I've been trying to record on FL Studio using ...</td>\n",
       "      <td>techsupport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.695148e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  Recommended wiki articles (including malware r...   \n",
       "1           1                       Another update on our future   \n",
       "2           2                     Help for my 92yo blind grandad   \n",
       "3           3  Moving countries and needing to be sure “illeg...   \n",
       "4           4   Insanely High PC DPC-Count Latency - How To Fix?   \n",
       "\n",
       "                                            selftext    subreddit  \\\n",
       "0  ## Check out these recommended threads on our ...  techsupport   \n",
       "1  Dear /r/TechSupport visitors and subscribers,\\...  techsupport   \n",
       "2  Hi Reddit I hope this might reach the right pe...  techsupport   \n",
       "3  Heyho, I’m moving countries in a couple of mon...  techsupport   \n",
       "4  I've been trying to record on FL Studio using ...  techsupport   \n",
       "\n",
       "                                            comments   created_utc  \n",
       "0                                                NaN  1.627263e+09  \n",
       "1  Discord is definitely not a proper alternative...  1.690183e+09  \n",
       "2  I know this isn't what you want to hear, but I...  1.695145e+09  \n",
       "3  Most data isn’t deleted when you click delete....  1.695140e+09  \n",
       "4                                                NaN  1.695148e+09  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.read_csv('reddit_only_dataset.csv')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab736523-aaae-40b6-a97e-6c9566496dd9",
   "metadata": {},
   "source": [
    "I initiated my openai connection, of course for security purposes it will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a194ad2d-d973-48fe-a8a7-309b7feb792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = 'MY_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc81484e-c1cf-48b4-94f4-bc288739dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_token_usage = 0\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ba244-b0eb-433f-af1d-fcf7080e4600",
   "metadata": {},
   "source": [
    "The for loop below is how I feed in the questions from the selftext column in my data to ask to openai and will them into the new column chatgpt_response once I get answer from openai.\n",
    "\n",
    "Some things to note is that I had to use iloc to do a few hundred calls at a time to make sure I wasnt racking up an enourmous bill and wouldnt face time out issues. Also I would have to go back into certain locations after the fact for several chunks of rows that the api misseed.\n",
    "\n",
    "Furthermore, I also decided to track total token use after every call and count each time it was called to monitor the speed of the requests, the amount of token per request, and see how far along I was in the code instead of running blind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a60319ea-2d68-4bb6-a1ae-fbdba9a37f99",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'null_response_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n_/367n_sc56lj0q1ddbd82d5qw0000gn/T/ipykernel_2309/881677309.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnull_response_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selftext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'null_response_df' is not defined"
     ]
    }
   ],
   "source": [
    "#For every row in the dataframe we are going to run through it\n",
    "for index, row in merged_df.iloc[1:5000].iterrows():\n",
    "    #keep track of how long things run\n",
    "    count += 1\n",
    "    #take the column selftext as the question for chatgpt\n",
    "    question = row['selftext']\n",
    "    \n",
    "    #Try catch block so we dont break from our for loop\n",
    "    try:\n",
    "        # Make an API request\n",
    "        chatgpt_response = openai.Completion.create(\n",
    "            #Using text-babbage-001 since it was extremly cheap\n",
    "            model='text-babbage-001',\n",
    "            prompt=question,\n",
    "            temperature = 0.6,\n",
    "            max_tokens=50  # Adjust max tokens as needed\n",
    "        )\n",
    "        \n",
    "        # extract and save response\n",
    "        merged_df.at[index, 'chatgpt_response'] = chatgpt_response.choices[0].text.strip()\n",
    "        \n",
    "        #monitor the amount of tokens used\n",
    "        token_usage = chatgpt_response['usage']['total_tokens']\n",
    "        total_token_usage += token_usage\n",
    "        \n",
    "        #I had to add this so I could slow down my api request calls to avoid a big bill\n",
    "        if count % 60 == 0:\n",
    "            time.sleep(60)  # Wait for 60 seconds every 60 requests\n",
    "        \n",
    "        # I had to add this as well to slow down the requests\n",
    "        if total_token_usage >= 250000:\n",
    "            time.sleep(60)  # Wait for 60 seconds\n",
    "        print(f\"Row: {count} is complete\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        #Notify me when a row doesnt work as intended\n",
    "        print(f\"Error processing row {index}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "#print the total token usage after processing all requests\n",
    "print(f\"Total Token Usage: {total_token_usage}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b8003-b014-4d08-a3f5-16f37c078ae9",
   "metadata": {},
   "source": [
    "At this point I want to check to see my null value to see which rows get skipped over. After checking how many rows didnt get input into chatgpt I would resort the rows and then run an iloc on the specific rows this time on the above for loop to re-ask the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b0d621-e9ed-45a8-8919-2008d6a04a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960bb8b-463b-4b79-a7ec-d0001a46c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.sort_values(by='chatgpt_response', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4171f60-d553-4ec5-827c-001f484c481e",
   "metadata": {},
   "source": [
    "Once I was done and was able to obtain the entirety of the data I needed I saved and exported it into a new excel sheet and decided to move into the next notebook for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b7bc4-3e9a-48f5-ae4e-349231d9a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('reddit_chatgpt_full_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
